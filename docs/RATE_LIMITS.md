# Rate Limits e OtimizaÃ§Ã£o de API

## Problema Identificado

O sistema estava funcionando perfeitamente (criou e removeu seÃ§Ãµes com sucesso), mas depois parou de responder com erro **"Failed to get response"**.

### Causa Raiz

**Rate Limits da API Groq (2 tipos):**

#### 1. Limite por Dia (TPD - Tokens Per Day)
- **Limite:** 100.000 tokens por dia (plano gratuito)
- **Aplica-se a:** Todos os modelos compartilham esse limite
- **Tempo de reset:** 24 horas apÃ³s inÃ­cio do dia
- **Erro:** HTTP 429 - `rate_limit_exceeded` (tokens per day)
- **SoluÃ§Ã£o:** Aguarde prÃ³ximo dia ou troque de modelo (economiza tokens)

#### 2. Limite por Minuto (TPM - Tokens Per Minute)
- **Llama 3.3 70B:** ~14.000 tokens por minuto
- **Llama 3.1 8B:** ~6.000 tokens por minuto
- **Tempo de reset:** ~5-30 segundos
- **Erro:** HTTP 429 - `rate_limit_exceeded` (tokens per minute)
- **SoluÃ§Ã£o:** Aguarde alguns segundos antes de enviar prÃ³xima mensagem

**âš ï¸ Importante:** Ambos os limites sÃ£o independentes! VocÃª pode atingir limite por minuto mesmo tendo tokens disponÃ­veis no dia.

## SoluÃ§Ãµes Implementadas

### 1. DetecÃ§Ã£o e Mensagem AmigÃ¡vel

```typescript
// components/AIChat.tsx
if (response.status === 429 && errorText.includes('rate_limit_exceeded')) {
  const match = errorText.match(/Please try again in (\d+m\d+)/);
  const waitTime = match ? match[1] : 'alguns minutos';
  throw new Error(`â±ï¸ Limite de uso da API atingido. Por favor, aguarde ${waitTime} ou use outra API key.`);
}
```

**BenefÃ­cio:** UsuÃ¡rio sabe exatamente quanto tempo esperar.

### 2. OtimizaÃ§Ã£o de Tokens

```typescript
// Antes: enviava TODO o histÃ³rico
messages.filter((m) => !m.isLoading)

// Depois: envia apenas Ãºltimas 10 mensagens
messages.filter((m) => !m.isLoading).slice(-10)
```

**Economia:** ~70-80% dos tokens em conversas longas.

### 3. Contexto Reduzido

O sistema jÃ¡ otimiza o contexto:
- Envia apenas IDs e tÃ­tulos das seÃ§Ãµes
- NÃ£o envia conteÃºdo completo das seÃ§Ãµes
- Usa comandos curtos (CRIAR, EDITAR, etc.)

## âš¡ Sistema de Fallback AutomÃ¡tico (NOVO!)

### Funcionalidades Implementadas

#### 1. Dropdown de SeleÃ§Ã£o de Modelo
- **Llama 3.3 70B (Premium)** - Melhor qualidade, mais detalhado
- **Llama 3.1 8B (RÃ¡pido)** - Mais econÃ´mico, 3x mais rÃ¡pido
- SeleÃ§Ã£o salva automaticamente no localStorage
- Conversa mantida ao trocar de modelo

#### 2. DetecÃ§Ã£o AutomÃ¡tica Inicial
Ao abrir o chat:
- Sistema testa automaticamente se modelo premium estÃ¡ disponÃ­vel
- Se atingiu rate limit â†’ troca automaticamente para 8B
- Mensagem de boas-vindas indica qual modelo estÃ¡ ativo
- NÃ£o bloqueia a UI durante teste

#### 3. Fallback Durante ConversaÃ§Ã£o
Se rate limit acontecer durante uso:
- Sistema detecta erro 429
- Troca automaticamente de 70B â†’ 8B
- Mostra mensagem: "âš¡ Modelo premium atingiu limite. Mudando automaticamente para Llama 3.1 8B"
- UsuÃ¡rio pode reenviar mensagem com novo modelo
- Pode voltar manualmente para 70B quando resetar

#### 4. PersistÃªncia de PreferÃªncia
- Modelo selecionado Ã© salvo no navegador
- PrÃ³xima sessÃ£o usa Ãºltimo modelo escolhido
- Reset manual disponÃ­vel via dropdown

### ComparaÃ§Ã£o de Modelos

| CaracterÃ­stica | Llama 3.3 70B | Llama 3.1 8B |
|----------------|---------------|--------------|
| Qualidade | â­â­â­â­â­ | â­â­â­â­ |
| Velocidade | Normal | 3x mais rÃ¡pido |
| **Limite por Minuto** | **~14K tokens/min** | **~6K tokens/min** |
| **Limite por Dia** | **100K tokens/dia (compartilhado)** | **100K tokens/dia (compartilhado)** |
| Tokens por msg | ~500 | ~200 |
| Custo por token | Maior | Menor |
| Comandos GDD | âœ… Funciona | âœ… Funciona |
| ExplicaÃ§Ãµes | Mais detalhadas | Mais diretas |

**âš ï¸ AtenÃ§Ã£o:**
- **Limite por dia** Ã© compartilhado entre todos os modelos (usar 70B consome do mesmo pool de 100K)
- **Limite por minuto** Ã© individual por modelo
- Se enviar mensagens muito rÃ¡pidas, pode atingir limite por minuto mesmo com tokens disponÃ­veis no dia!

**Ambos os modelos:**
- âœ… Executam comandos CRIAR/SUBSECAO/EDITAR/REMOVER
- âœ… Entendem sistema de referÃªncias $[section]
- âœ… Seguem fluxo de 2 passos (propor â†’ confirmar â†’ executar)
- âœ… Criam conteÃºdo markdown rico

## Como Evitar Rate Limits

### OpÃ§Ã£o 1: Usar Modelo 8B (RECOMENDADO)
- Troca automÃ¡tica quando 70B atinge limite
- Mesma funcionalidade, apenas mais direto
- 3x mais rÃ¡pido = gasta menos tokens por minuto
- Use dropdown para trocar manualmente

### OpÃ§Ã£o 2: Aguardar Reset
Simplesmente espere o tempo indicado na mensagem de erro (~35 min).

### OpÃ§Ã£o 3: Upgrade para Dev Tier
- Acesse: https://console.groq.com/settings/billing
- Plano Dev Tier: ~$0.59/milhÃ£o de tokens
- Limite muito maior

### OpÃ§Ã£o 4: MÃºltiplas API Keys
Crie variÃ¡veis de ambiente alternativas:
```bash
# .env.local
NEXT_PUBLIC_GROQ_API_KEY=sua-key-principal
GROQ_API_KEY_BACKUP=sua-key-secundaria
```

### OpÃ§Ã£o 5: Usar Outro Provider
Edite `.env.local`:
```bash
# OpÃ§Ãµes: groq, openai, claude
NEXT_PUBLIC_AI_PROVIDER=openai
NEXT_PUBLIC_OPENAI_API_KEY=sua-key-openai
```

## Monitoramento

Para ver uso atual de tokens:
1. Acesse: https://console.groq.com/
2. Dashboard â†’ Usage
3. Veja grÃ¡fico de tokens usados por dia

## Logs de Erro

Os erros 429 sÃ£o logados no terminal com detalhes completos:
```
Error: API error: 429 - {
  "error": {
    "message": "Rate limit reached...",
    "type": "tokens",
    "code": "rate_limit_exceeded"
  }
}
```

## âš ï¸ Erro Comum: "Aguarde alguns segundos"

Se vocÃª receber:
```
â±ï¸ Limite de requisiÃ§Ãµes por minuto atingido. Aguarde 4.87s e tente novamente.

ğŸ’¡ Dica: O modelo estÃ¡ processando muitas mensagens rapidamente. DÃª um tempo!
```

**Isso NÃƒO significa que acabaram seus tokens do dia!**

**O que aconteceu:**
- VocÃª enviou mensagens muito rÃ¡pidas (ex: 3 mensagens em 10 segundos)
- Atingiu limite de **tokens por minuto** (TPM)
- Modelo 8B: mÃ¡x 6.000 tokens/minuto
- Modelo 70B: mÃ¡x 14.000 tokens/minuto

**SoluÃ§Ã£o:**
- â° Aguarde 5-30 segundos
- ğŸ”„ Reenvie a mensagem
- ğŸ’¡ NÃ£o precisa trocar de modelo!

## PrevenÃ§Ã£o Futura

### Comandos Curtos
âœ… Use comandos diretos:
- "Cria seÃ§Ã£o X"
- "Remove Y"
- "Edita Z"

âŒ Evite conversas muito longas sem necessidade.

### EspaÃ§amento de Mensagens
âœ… Aguarde resposta antes de enviar prÃ³xima mensagem
âŒ NÃ£o envie 3-4 mensagens seguidas rapidamente

### Clear Context
Se notar lentidÃ£o, recarregue a pÃ¡gina para limpar histÃ³rico.

### Batch Operations
Agrupe operaÃ§Ãµes:
- "Cria 3 seÃ§Ãµes: A, B, C" âœ…
- vs 3 mensagens separadas âŒ

## Status Atual

âœ… Sistema funcional (testado com criar/remover)  
âœ… DetecÃ§Ã£o de rate limit implementada  
âœ… Mensagens amigÃ¡veis de erro  
âœ… OtimizaÃ§Ã£o de histÃ³rico (Ãºltimas 10 msgs)  
âœ… **Dropdown de seleÃ§Ã£o de modelo (70B â†” 8B)**  
âœ… **Fallback automÃ¡tico quando rate limit**  
âœ… **DetecÃ§Ã£o inicial automÃ¡tica de disponibilidade**  
âœ… **PersistÃªncia de preferÃªncia de modelo**  
âœ… **Modelo 8B configurado para comandos GDD**  

## PrÃ³ximos Testes

Com sistema de fallback ativo:
- [ ] Testar troca manual de modelo durante conversa
- [ ] Verificar se conversa persiste ao trocar modelo
- [ ] Testar detecÃ§Ã£o automÃ¡tica inicial
- [ ] Validar que modelo 8B executa todos os comandos corretamente
- [ ] CRIAR seÃ§Ãµes raiz com ambos os modelos
- [ ] SUBSECAO com pais diferentes
- [ ] EDITAR conteÃºdo de seÃ§Ãµes
- [ ] REMOVER mÃºltiplas seÃ§Ãµes
- [ ] Comandos em batch

## Fluxo do Sistema

```mermaid
graph TD
    A[UsuÃ¡rio abre chat] --> B{Teste modelo premium}
    B -->|DisponÃ­vel| C[Usa Llama 3.3 70B]
    B -->|Rate limit| D[Usa Llama 3.1 8B automaticamente]
    C --> E[Durante conversa]
    D --> E
    E --> F{Rate limit?}
    F -->|NÃ£o| G[Continua normalmente]
    F -->|Sim| H[Fallback automÃ¡tico para 8B]
    H --> I[Mostra mensagem ao usuÃ¡rio]
    I --> J[UsuÃ¡rio pode reenviar ou trocar manualmente]
    G --> K[UsuÃ¡rio pode trocar manualmente via dropdown]
    K --> L[PreferÃªncia salva no localStorage]
```
